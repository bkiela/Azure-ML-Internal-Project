{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "from itertools import product\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "source_dir = r'C:\\Users\\bartlomiej.kielan\\Desktop\\codes\\azure-ml-internal-project\\data\\aggregated'\n",
    "csv_files = [file for file in os.listdir(source_dir) if file.endswith('.csv')]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in csv_files:\n",
    "    filepath = os.path.join(source_dir, filename)\n",
    "    temp_df = pd.read_csv(filepath)\n",
    "    dfs.append(temp_df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "country_region = combined_df['Country_Region']\n",
    "combined_df.drop(['Country_Region'], axis=1, inplace=True)\n",
    "numeric_columns = combined_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "data_numeric = combined_df[numeric_columns]\n",
    "\n",
    "stand_scaler = preprocessing.MinMaxScaler()\n",
    "data_scaled = stand_scaler.fit_transform(data_numeric)\n",
    "\n",
    "def sdae_reduction(data_scaled, dim_num=2, hyperparameters=None):\n",
    "    global best_loss\n",
    "    global best_hyperparameters\n",
    "\n",
    "    num_layers_values, layer_dim_values, activation_ch, epoch_num, batch_size_num, learning_rate_num = hyperparameters\n",
    "\n",
    "    neurons_per_layer = [\n",
    "        max(int(layer_dim_values / (2**i)), 2)\n",
    "        for i in range(num_layers_values)\n",
    "    ]\n",
    "\n",
    "    encoder_input = layers.Input(shape=(data_scaled.shape[1],))\n",
    "    encoder_output = encoder_input\n",
    "    for dim in neurons_per_layer[:-1]:\n",
    "        encoder_output = layers.Dense(dim, activation=activation_ch)(encoder_output)\n",
    "\n",
    "    encoded_layer = layers.Dense(dim_num, activation=activation_ch)(encoder_output)\n",
    "    encoder = keras.models.Model(encoder_input, encoded_layer)\n",
    "\n",
    "    decoder_input = layers.Input(shape=(dim_num,))\n",
    "    decoder_output = decoder_input\n",
    "    for dim in reversed(neurons_per_layer[:-1]):\n",
    "        decoder_output = layers.Dense(dim, activation=activation_ch)(decoder_output)\n",
    "    decoder_output = layers.Dense(data_scaled.shape[1], activation='sigmoid')(decoder_output)\n",
    "    decoder = keras.models.Model(decoder_input, decoder_output)\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    autoencoder_output = decoder(encoded_layer)\n",
    "    autoencoder = keras.models.Model(autoencoder_input, autoencoder_output)\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate_num)\n",
    "    autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "    loss_values = []\n",
    "    epoch_numbers = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        history = autoencoder.fit(\n",
    "            data_scaled,\n",
    "            data_scaled,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size_num,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        loss_values.append(history.history['loss'][0])\n",
    "        epoch_numbers.append(epoch + 1)\n",
    "\n",
    "        plt.clf()\n",
    "        plt.plot(epoch_numbers, loss_values, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        if best_hyperparameters is None or history.history['loss'][0] < best_loss:\n",
    "            best_loss = history.history['loss'][0]\n",
    "            best_hyperparameters = {\n",
    "                'num_layers_values': num_layers_values,\n",
    "                'layer_dim_values': layer_dim_values,\n",
    "                'activation': activation_ch,\n",
    "                'epochs': epoch + 1,\n",
    "                'batch_size': batch_size_num,\n",
    "                'learning_rate': learning_rate_num\n",
    "            }\n",
    "            models_directory = r'C:\\Users\\bartlomiej.kielan\\Desktop\\codes\\azure-ml-internal-project\\pipeline\\training\\models'\n",
    "\n",
    "            model_filename = os.path.join(models_directory, 'best_ae_model.keras')\n",
    "\n",
    "            autoencoder.save(model_filename)\n",
    "\n",
    "    display.clear_output()\n",
    "\n",
    "hyperparameter_grid = product(\n",
    "    [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [10, 15, 20, 25, 30],\n",
    "    [\"relu\", \"sigmoid\", \"tanh\"],\n",
    "    range(100, 1001, 100),\n",
    "    [128],\n",
    "    np.linspace(0.0001, 0.1, num=20)\n",
    ")\n",
    "\n",
    "for hyperparameters in hyperparameter_grid:\n",
    "    num_layers_values, layer_dim_values, activation_ch, epoch_num, batch_size_num, learning_rate_num = hyperparameters\n",
    "    print(f\"Hyperparameters: {hyperparameters}\")\n",
    "\n",
    "    encoded_data = sdae_reduction(data_scaled, dim_num=2, hyperparameters=hyperparameters)\n",
    "    print(f'Best Loss: {best_loss}')\n",
    "    if best_hyperparameters:\n",
    "        print(f'Best Hyperparameters: {best_hyperparameters}')\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=['x', 'y'])\n",
    "encoded_df['Country_Region'] = country_region\n",
    "\n",
    "encoded_csv_directory = r'C:\\Users\\bartlomiej.kielan\\Desktop\\codes\\azure-ml-internal-project\\pipeline\\training\\hyperparameter_tuning'\n",
    "\n",
    "encoded_csv_path = os.path.join(encoded_csv_directory, 'encoded_data.csv')\n",
    "encoded_df.to_csv(encoded_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 7s 36ms/step - loss: -2051075994575437824.0000\n",
      "10/60 [====>.........................] - ETA: 4s - loss: -548773440782336.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bartlomiej.kielan\\Desktop\\codes\\azure-ml-internal-project\\pipeline\\training\\hyperparameter_tuning\\asd.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mget_event_loop()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     loop\u001b[39m.\u001b[39;49mrun_until_complete(main())\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining completed.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     91\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[1;32m---> 93\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once()\n\u001b[0;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[0;32m     95\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:129\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m     handle \u001b[39m=\u001b[39m ready\u001b[39m.\u001b[39mpopleft()\n\u001b[0;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handle\u001b[39m.\u001b[39m_cancelled:\n\u001b[1;32m--> 129\u001b[0m         handle\u001b[39m.\u001b[39;49m_run()\n\u001b[0;32m    130\u001b[0m handle \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     79\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callback, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args)\n\u001b[0;32m     81\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mSystemExit\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:205\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[1;34m(task, exc)\u001b[0m\n\u001b[0;32m    203\u001b[0m curr_task \u001b[39m=\u001b[39m curr_tasks\u001b[39m.\u001b[39mget(task\u001b[39m.\u001b[39m_loop)\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     step_orig(task, exc)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m curr_task \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "\u001b[1;32mc:\\Users\\bartlomiej.kielan\\Desktop\\codes\\azure-ml-internal-project\\pipeline\\training\\hyperparameter_tuning\\asd.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m encoder_output \u001b[39m=\u001b[39m encoder_input\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m neurons_per_layer[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     encoder_output \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mDense(dim, activation\u001b[39m=\u001b[39;49mactivation_ch)(encoder_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m encoded_layer \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDense(dim_num, activation\u001b[39m=\u001b[39mactivation_ch)(encoder_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bartlomiej.kielan/Desktop/codes/azure-ml-internal-project/pipeline/training/hyperparameter_tuning/asd.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m encoder \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mModel(encoder_input, encoded_layer)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:1063\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[39m# on symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\n\u001b[0;32m   1061\u001b[0m     \u001b[39mself\u001b[39m, inputs, args, kwargs, input_list\n\u001b[0;32m   1062\u001b[0m ):\n\u001b[1;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_functional_construction_call(\n\u001b[0;32m   1064\u001b[0m         inputs, args, kwargs, input_list\n\u001b[0;32m   1065\u001b[0m     )\n\u001b[0;32m   1067\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2603\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   2597\u001b[0m \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[0;32m   2598\u001b[0m     layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value\n\u001b[0;32m   2599\u001b[0m ):\n\u001b[0;32m   2600\u001b[0m     \u001b[39m# Check input assumptions set after layer building, e.g. input\u001b[39;00m\n\u001b[0;32m   2601\u001b[0m     \u001b[39m# shape.\u001b[39;00m\n\u001b[0;32m   2602\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2603\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_tensor_symbolic_call(\n\u001b[0;32m   2604\u001b[0m             inputs, input_masks, args, kwargs\n\u001b[0;32m   2605\u001b[0m         )\n\u001b[0;32m   2606\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   2607\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mDictWrapper\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2449\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m   2446\u001b[0m         keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature\n\u001b[0;32m   2447\u001b[0m     )\n\u001b[0;32m   2448\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_output_signature(\n\u001b[0;32m   2450\u001b[0m         inputs, args, kwargs, input_masks\n\u001b[0;32m   2451\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2506\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m   2498\u001b[0m \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_scope()):\n\u001b[0;32m   2499\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   2500\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   2501\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2504\u001b[0m         \u001b[39m# TODO(kaftan): do we maybe_build here, or have we already\u001b[39;00m\n\u001b[0;32m   2505\u001b[0m         \u001b[39m# done it?\u001b[39;00m\n\u001b[1;32m-> 2506\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_build(inputs)\n\u001b[0;32m   2507\u001b[0m         inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[0;32m   2508\u001b[0m         outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:3035\u001b[0m, in \u001b[0;36mLayer._maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3030\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild, \u001b[39m\"\u001b[39m\u001b[39m_is_default\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   3031\u001b[0m     \u001b[39m# Any setup work performed only once should happen in an\u001b[39;00m\n\u001b[0;32m   3032\u001b[0m     \u001b[39m# `init_scope` to avoid creating symbolic Tensors that will\u001b[39;00m\n\u001b[0;32m   3033\u001b[0m     \u001b[39m# later pollute any eager operations.\u001b[39;00m\n\u001b[0;32m   3034\u001b[0m     \u001b[39mwith\u001b[39;00m tf_utils\u001b[39m.\u001b[39mmaybe_init_scope(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 3035\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(input_shapes)\n\u001b[0;32m   3036\u001b[0m \u001b[39m# We must set also ensure that the layer is marked as built, and the\u001b[39;00m\n\u001b[0;32m   3037\u001b[0m \u001b[39m# build shape is stored since user defined build functions may not\u001b[39;00m\n\u001b[0;32m   3038\u001b[0m \u001b[39m# be calling `super.build()`\u001b[39;00m\n\u001b[0;32m   3039\u001b[0m Layer\u001b[39m.\u001b[39mbuild(\u001b[39mself\u001b[39m, input_shapes)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:154\u001b[0m, in \u001b[0;36mDense.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe last dimension of the inputs to a Dense layer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshould be defined. Found None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull input shape received: \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m     )\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m InputSpec(min_ndim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axes\u001b[39m=\u001b[39m{\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: last_dim})\n\u001b[1;32m--> 154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_weight(\n\u001b[0;32m    155\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mkernel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    156\u001b[0m     shape\u001b[39m=\u001b[39;49m[last_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munits],\n\u001b[0;32m    157\u001b[0m     initializer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_initializer,\n\u001b[0;32m    158\u001b[0m     regularizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_regularizer,\n\u001b[0;32m    159\u001b[0m     constraint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_constraint,\n\u001b[0;32m    160\u001b[0m     dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype,\n\u001b[0;32m    161\u001b[0m     trainable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bias:\n\u001b[0;32m    164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[0;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    166\u001b[0m         shape\u001b[39m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         trainable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    174\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:712\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[39mif\u001b[39;00m layout:\n\u001b[0;32m    710\u001b[0m     getter \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(getter, layout\u001b[39m=\u001b[39mlayout)\n\u001b[1;32m--> 712\u001b[0m variable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_variable_with_custom_getter(\n\u001b[0;32m    713\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    714\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    715\u001b[0m     \u001b[39m# TODO(allenl): a `make_variable` equivalent should be added as a\u001b[39;49;00m\n\u001b[0;32m    716\u001b[0m     \u001b[39m# `Trackable` method.\u001b[39;49;00m\n\u001b[0;32m    717\u001b[0m     getter\u001b[39m=\u001b[39;49mgetter,\n\u001b[0;32m    718\u001b[0m     \u001b[39m# Manage errors in Layer rather than Trackable.\u001b[39;49;00m\n\u001b[0;32m    719\u001b[0m     overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    720\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m    721\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    722\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    723\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    724\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m    725\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections_arg,\n\u001b[0;32m    726\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    727\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m    728\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    729\u001b[0m )\n\u001b[0;32m    730\u001b[0m \u001b[39mif\u001b[39;00m regularizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[39m# TODO(fchollet): in the future, this should be handled at the\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[39m# level of variable creation, and weight regularization losses\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[39m# should be variable attributes.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     name_in_scope \u001b[39m=\u001b[39m variable\u001b[39m.\u001b[39mname[: variable\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:488\u001b[0m, in \u001b[0;36mTrackable._add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    478\u001b[0m   \u001b[39mif\u001b[39;00m (checkpoint_initializer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    479\u001b[0m       \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(initializer, CheckpointInitialValueCallable) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    480\u001b[0m            (initializer\u001b[39m.\u001b[39mrestore_uid \u001b[39m>\u001b[39m checkpoint_initializer\u001b[39m.\u001b[39mrestore_uid))):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[39m# then we'll catch that when we call _track_trackable. So this is\u001b[39;00m\n\u001b[0;32m    486\u001b[0m     \u001b[39m# \"best effort\" to set the initializer with the highest restore UID.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m     initializer \u001b[39m=\u001b[39m checkpoint_initializer\n\u001b[1;32m--> 488\u001b[0m new_variable \u001b[39m=\u001b[39m getter(\n\u001b[0;32m    489\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    490\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    491\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    492\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m    493\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_for_getter)\n\u001b[0;32m    495\u001b[0m \u001b[39m# If we set an initializer and the variable processed it, tracking will not\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[39m# assign again. It will add this variable to our dependencies, and if there\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m# is a non-trivial restoration queued, it will handle that. This also\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[39m# handles slot variables.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overwrite \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(new_variable, Trackable):\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:137\u001b[0m, in \u001b[0;36mmake_variable\u001b[1;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner, layout, experimental_enable_variable_lifting)\u001b[0m\n\u001b[0;32m    130\u001b[0m     use_resource \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m layout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# In theory, in `use_resource` is True and `collections` is empty\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[39m# (that is to say, in TF2), we can use tf.Variable.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[39m# However, this breaks legacy (Estimator) checkpoints because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[39m# it changes variable names. Remove this when V1 is fully deprecated.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m tf1\u001b[39m.\u001b[39;49mVariable(\n\u001b[0;32m    138\u001b[0m         initial_value\u001b[39m=\u001b[39;49minit_val,\n\u001b[0;32m    139\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    140\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    141\u001b[0m         caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    142\u001b[0m         dtype\u001b[39m=\u001b[39;49mvariable_dtype,\n\u001b[0;32m    143\u001b[0m         validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    144\u001b[0m         constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    145\u001b[0m         use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m    146\u001b[0m         collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m    147\u001b[0m         synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    148\u001b[0m         aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m    149\u001b[0m         shape\u001b[39m=\u001b[39;49mvariable_shape \u001b[39mif\u001b[39;49;00m variable_shape \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    150\u001b[0m         experimental_enable_variable_lifting\u001b[39m=\u001b[39;49mexperimental_enable_variable_lifting,  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m    151\u001b[0m     )\n\u001b[0;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     \u001b[39mreturn\u001b[39;00m dtensor\u001b[39m.\u001b[39mDVariable(\n\u001b[0;32m    154\u001b[0m         initial_value\u001b[39m=\u001b[39minit_val,\n\u001b[0;32m    155\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m         shape\u001b[39m=\u001b[39mvariable_shape \u001b[39mif\u001b[39;00m variable_shape \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    165\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:194\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39m@traceback_utils\u001b[39m\u001b[39m.\u001b[39mfilter_traceback\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    193\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_variable_call\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_call):\n\u001b[1;32m--> 194\u001b[0m     variable_call \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    195\u001b[0m     \u001b[39mif\u001b[39;00m variable_call \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m       \u001b[39mreturn\u001b[39;00m variable_call\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_v1.py:305\u001b[0m, in \u001b[0;36mVariableV1._variable_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape, experimental_enable_variable_lifting, expected_shape, collections, use_resource, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m aggregation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m   aggregation \u001b[39m=\u001b[39m variables\u001b[39m.\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE\n\u001b[1;32m--> 305\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\n\u001b[0;32m    306\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m    307\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    308\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    309\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    310\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    311\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[0;32m    312\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    313\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[0;32m    314\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    315\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    316\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m    317\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    318\u001b[0m     experimental_enable_variable_lifting\u001b[39m=\u001b[39;49mexperimental_enable_variable_lifting,\n\u001b[0;32m    319\u001b[0m     expected_shape\u001b[39m=\u001b[39;49mexpected_shape,\n\u001b[0;32m    320\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m    321\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m    322\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_v1.py:298\u001b[0m, in \u001b[0;36mVariableV1._variable_call.<locals>.<lambda>\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m VariableV1:\n\u001b[0;32m    297\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: default_variable_creator(\u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    299\u001b[0m \u001b[39mfor\u001b[39;00m _, getter \u001b[39min\u001b[39;00m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_variable_creator_stack:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    300\u001b[0m   previous_getter \u001b[39m=\u001b[39m variables\u001b[39m.\u001b[39m_make_getter(getter, previous_getter)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\ref_variable.py:70\u001b[0m, in \u001b[0;36mdefault_variable_creator\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m use_resource:\n\u001b[0;32m     69\u001b[0m   distribute_strategy \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdistribute_strategy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> 70\u001b[0m   \u001b[39mreturn\u001b[39;00m resource_variable_ops\u001b[39m.\u001b[39;49mResourceVariable(\n\u001b[0;32m     71\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m     72\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m     73\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m     74\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m     75\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m     76\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m     77\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m     78\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m     79\u001b[0m       variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[0;32m     80\u001b[0m       import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[0;32m     81\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[0;32m     82\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m     83\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m     84\u001b[0m       shape\u001b[39m=\u001b[39;49mshape)\n\u001b[0;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m   \u001b[39mreturn\u001b[39;00m RefVariable(\n\u001b[0;32m     87\u001b[0m       initial_value\u001b[39m=\u001b[39minitial_value,\n\u001b[0;32m     88\u001b[0m       trainable\u001b[39m=\u001b[39mtrainable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m       aggregation\u001b[39m=\u001b[39maggregation,\n\u001b[0;32m    100\u001b[0m       shape\u001b[39m=\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:197\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m variable_call \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m variable_call\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(VariableMetaclass, \u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1810\u001b[0m, in \u001b[0;36mResourceVariable.__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape, handle, experimental_enable_variable_lifting)\u001b[0m\n\u001b[0;32m   1805\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_handle(trainable\u001b[39m=\u001b[39mtrainable,\n\u001b[0;32m   1806\u001b[0m                          shape\u001b[39m=\u001b[39mshape,\n\u001b[0;32m   1807\u001b[0m                          dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1808\u001b[0m                          handle\u001b[39m=\u001b[39mhandle)\n\u001b[0;32m   1809\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1810\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_from_args(\n\u001b[0;32m   1811\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   1812\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   1813\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m   1814\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   1815\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1816\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1817\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   1818\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   1819\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m   1820\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1821\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[0;32m   1822\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   1823\u001b[0m       experimental_enable_variable_lifting\u001b[39m=\u001b[39;49mexperimental_enable_variable_lifting,\n\u001b[0;32m   1824\u001b[0m       )\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1994\u001b[0m, in \u001b[0;36mResourceVariable._init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape, validate_shape, experimental_enable_variable_lifting)\u001b[0m\n\u001b[0;32m   1992\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mInitializer\u001b[39m\u001b[39m\"\u001b[39m), device_context_manager(\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1993\u001b[0m   \u001b[39mif\u001b[39;00m init_from_fn:\n\u001b[1;32m-> 1994\u001b[0m     initial_value \u001b[39m=\u001b[39m initial_value()\n\u001b[0;32m   1995\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(initial_value, trackable\u001b[39m.\u001b[39mCheckpointInitialValue):\n\u001b[0;32m   1996\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_initialize_trackable()\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:647\u001b[0m, in \u001b[0;36mVarianceScaling.__call__\u001b[1;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m     _ensure_keras_seeded()\n\u001b[0;32m    640\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mcall_with_layout(\n\u001b[0;32m    641\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_init_val,\n\u001b[0;32m    642\u001b[0m         layout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         nonce\u001b[39m=\u001b[39mnonce,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_init_val(shape\u001b[39m=\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49mdtype, nonce\u001b[39m=\u001b[39;49mnonce)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:672\u001b[0m, in \u001b[0;36mVarianceScaling._generate_init_val\u001b[1;34m(self, shape, dtype, nonce)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     limit \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m \u001b[39m*\u001b[39m scale)\n\u001b[1;32m--> 672\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_random_generator\u001b[39m.\u001b[39;49mrandom_uniform(\n\u001b[0;32m    673\u001b[0m         shape, \u001b[39m-\u001b[39;49mlimit, limit, dtype, nonce\n\u001b[0;32m    674\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:2102\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2100\u001b[0m     \u001b[39mif\u001b[39;00m nonce:\n\u001b[0;32m   2101\u001b[0m         seed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2102\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mstateless_uniform(\n\u001b[0;32m   2103\u001b[0m         shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   2104\u001b[0m         minval\u001b[39m=\u001b[39;49mminval,\n\u001b[0;32m   2105\u001b[0m         maxval\u001b[39m=\u001b[39;49mmaxval,\n\u001b[0;32m   2106\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   2107\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m   2108\u001b[0m     )\n\u001b[0;32m   2109\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\n\u001b[0;32m   2110\u001b[0m     shape\u001b[39m=\u001b[39mshape,\n\u001b[0;32m   2111\u001b[0m     minval\u001b[39m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2114\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2115\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bartlomiej.kielan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/60 [=======>......................] - ETA: 2s - loss: -5133523153321984.0000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 7s 35ms/step - loss: -1906999.12508.00\n",
      "60/60 [==============================] - 7s 27ms/step - loss: -11137930743580196864.0000\n",
      "60/60 [==============================] - 7s 13ms/step - loss: -9052491437223968768.0000\n",
      "60/60 [==============================] - 5s 11ms/step - loss: -3615394618121650176.0000\n",
      "60/60 [==============================] - 7s 11ms/step - loss: nan                  \n",
      "60/60 [==============================] - 6s 12ms/step - loss: nan                  \n",
      "60/60 [==============================] - 6s 34ms/step - loss: nan504.00\n",
      "60/60 [==============================] - 6s 35ms/step - loss: 48317255680.0000\n",
      "60/60 [==============================] - 8s 39ms/step - loss: 1824685948928.000000\n",
      "60/60 [==============================] - 8s 52ms/step - loss: nan43886624.\n",
      "60/60 [==============================] - 8s 36ms/step - loss: -1253007719137280.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -198399.312500\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -120917038918008832.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -3972075681218560.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -326597078488711168.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -109558233399558144.0000\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -4871404958559240192.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -883658459948515328.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -222684674089025536.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -7509733686942629888.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -1885136.1250\n",
      "60/60 [==============================] - 12s 63ms/step - loss: 1914046208.0000\n",
      "60/60 [==============================] - 11s 55ms/step - loss: 86028288000.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -2811184.5000.000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -2838046475026432.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -8788132558022901760.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: nan01681235968.00\n",
      "60/60 [==============================] - 10s 51ms/step - loss: nan65600.0000.\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -2152006549504.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: nan074752.00\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -125974502965248.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -10906134219063296.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -728453.625000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -18870587355037696.0000\n",
      "60/60 [==============================] - 12s 59ms/step - loss: -932943.81254.000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -154192779080105984.0000\n",
      "60/60 [==============================] - 11s 54ms/step - loss: -91195452912828416.0000\n",
      "60/60 [==============================] - 10s 48ms/step - loss: -15284338625504870400.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -1094868321452425216.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -6990638.0000.0000\n",
      "60/60 [==============================] - 13s 66ms/step - loss: -6786442502598557696.0000\n",
      "60/60 [==============================] - 10s 55ms/step - loss: nan31250000\n",
      "60/60 [==============================] - 11s 40ms/step - loss: -28387686.0000000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -198108957120659456.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: nan8834057216.00\n",
      "60/60 [==============================] - 9s 34ms/step - loss: nan98896640.00 \n",
      "60/60 [==============================] - 10s 39ms/step - loss: nan\n",
      "60/60 [==============================] - 10s 46ms/step - loss: nan77840.0000.\n",
      "60/60 [==============================] - 10s 44ms/step - loss: nan9936.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -4329129377792.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -199436.01560\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -64904140685312.0000\n",
      "60/60 [==============================] - 10s 55ms/step - loss: -597602.0000.000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -174740057242992640.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -4238966291169280.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -224677401475416064.0000\n",
      "60/60 [==============================] - 11s 34ms/step - loss: -244241435459584.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: -127183310303526912.0000\n",
      "60/60 [==============================] - 9s 27ms/step - loss: -3033755990685122560.0000\n",
      "60/60 [==============================] - 10s 33ms/step - loss: -3975299683386916864.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -166510000.0000   \n",
      "60/60 [==============================] - 10s 43ms/step - loss: nan15325992960.00\n",
      "60/60 [==============================] - 9s 43ms/step - loss: -19410701310943559680.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -8303384819371343872.0000\n",
      "60/60 [==============================] - 8s 34ms/step - loss: -4113061618399576064.0000\n",
      "60/60 [==============================] - 8s 36ms/step - loss: nan6850816.\n",
      "60/60 [==============================] - 9s 43ms/step - loss: nan103489028096.00\n",
      "60/60 [==============================] - 10s 41ms/step - loss: nan00006.0000\n",
      "60/60 [==============================] - 8s 38ms/step - loss: -1441027063808.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: nan392.000000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: 101660581888.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -393858802253824.0000\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -28284071188627456.0000\n",
      "60/60 [==============================] - 10s 49ms/step - loss: -317782449192960.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -37756164145938432.0000\n",
      "60/60 [==============================] - 10s 48ms/step - loss: -12939345967185920.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -753146292292550656.0000\n",
      "60/60 [==============================] - 8s 36ms/step - loss: -943591120791666688.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -479133254323011584.0000\n",
      "60/60 [==============================] - 11s 40ms/step - loss: -217597285326913536.0000\n",
      "60/60 [==============================] - 10s 51ms/step - loss: -6786219851493933056.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -13493007684462444544.0000\n",
      "60/60 [==============================] - 9s 29ms/step - loss: nan56454080.0\n",
      "60/60 [==============================] - 12s 35ms/step - loss: -2463530.250000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -7262176444925607936.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: nan               \n",
      "60/60 [==============================] - 10s 35ms/step - loss: nan944889856.00\n",
      "60/60 [==============================] - 12s 56ms/step - loss: nan7383165440.0\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -7937.0112\n",
      "60/60 [==============================] - 12s 41ms/step - loss: nan816.00\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -24144815063040.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -21571972366336.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -1204965.25000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -44718555240857600.0000\n",
      "60/60 [==============================] - 9s 28ms/step - loss: -16841733924978688.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -96972922660323328.0000\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -4589620306116608.0000\n",
      "60/60 [==============================] - 8s 23ms/step - loss: -399484528927703040.0000\n",
      "60/60 [==============================] - 8s 25ms/step - loss: -1317682222517452800.0000\n",
      "60/60 [==============================] - 8s 23ms/step - loss: -1864261822190714880.0000\n",
      "60/60 [==============================] - 8s 37ms/step - loss: -2514080.750000.0\n",
      "60/60 [==============================] - 10s 48ms/step - loss: -2122165.50000000\n",
      "60/60 [==============================] - 9s 43ms/step - loss: -18671075232101433344.0000\n",
      "60/60 [==============================] - 7s 25ms/step - loss: nan2.00000469504.\n",
      "60/60 [==============================] - 9s 29ms/step - loss: nan9.0000       \n",
      "60/60 [==============================] - 6s 22ms/step - loss: nan9.00\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -14924136.0000\n",
      "60/60 [==============================] - 7s 35ms/step - loss: 73721896960.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: 86134521856.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: 1145539133440.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -791298130837504.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -72203047534592.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -13703200532070400.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -25425259352031232.0000\n",
      "60/60 [==============================] - 12s 64ms/step - loss: -71818626352545792.0000\n",
      "60/60 [==============================] - 11s 64ms/step - loss: -50068276310441984.0000\n",
      "60/60 [==============================] - 12s 70ms/step - loss: -273016004261445632.0000\n",
      "60/60 [==============================] - 12s 90ms/step - loss: -357959616878870528.0000\n",
      "60/60 [==============================] - 13s 56ms/step - loss: -280839510529409024.0000\n",
      "60/60 [==============================] - 12s 49ms/step - loss: -1528168330492379136.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -2127591.7500.\n",
      "60/60 [==============================] - 12s 43ms/step - loss: -859523492524064768.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: nan.00\n",
      "60/60 [==============================] - 12s 46ms/step - loss: 696089280.0000\n",
      "60/60 [==============================] - 12s 39ms/step - loss: nan7546502144.00\n",
      "60/60 [==============================] - 12s 41ms/step - loss: nan96332750848.00\n",
      "60/60 [==============================] - 11s 40ms/step - loss: nan25324800.000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: nan825814016.00\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -3497.08540\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -185930224762880.0000\n",
      "60/60 [==============================] - 12s 53ms/step - loss: nan630464.00\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -104028864249856.0000\n",
      "60/60 [==============================] - 10s 32ms/step - loss: -597514.1875\n",
      "60/60 [==============================] - 10s 35ms/step - loss: 5404183552.0000\n",
      "60/60 [==============================] - 10s 31ms/step - loss: -31970836755775488.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -38164817399250944.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -596811131985592320.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -1121839341681770496.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -826704169946578944.0000\n",
      "60/60 [==============================] - 9s 27ms/step - loss: -26488818409867837440.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -1283206348233768960.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -4940136529923145728.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: -5755554144291127296.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: nan08.00\n",
      "60/60 [==============================] - 11s 45ms/step - loss: 52679811072.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -15767219.0000\n",
      "60/60 [==============================] - 8s 33ms/step - loss: nan5794749440.00\n",
      "60/60 [==============================] - 10s 35ms/step - loss: nan57344169984.00\n",
      "60/60 [==============================] - 8s 33ms/step - loss: -3729873698816.0000\n",
      "60/60 [==============================] - 11s 37ms/step - loss: -11642694541659602944.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: 124515120.00000\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -3399822560198656.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: 9003974656.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -31669175432773632.0000\n",
      "60/60 [==============================] - 8s 27ms/step - loss: 715844608.0000.00\n",
      "60/60 [==============================] - 10s 29ms/step - loss: -276429506829352960.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -23494734.0000\n",
      "60/60 [==============================] - 10s 54ms/step - loss: -1472058327981621248.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -201749989876039680.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -5106870871205609472.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -4539945083738259456.0000\n",
      "60/60 [==============================] - 9s 45ms/step - loss: nan689302335488.00\n",
      "60/60 [==============================] - 12s 45ms/step - loss: -20303100132418650112.0000\n",
      "60/60 [==============================] - 11s 38ms/step - loss: nan04.00\n",
      "60/60 [==============================] - 9s 36ms/step - loss: nan.000016617472.00\n",
      "60/60 [==============================] - 11s 39ms/step - loss: nan.000\n",
      "60/60 [==============================] - 12s 34ms/step - loss: -16375688.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -3554431.2500\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -3685.0076000616.00\n",
      "60/60 [==============================] - 11s 42ms/step - loss: nan60704.00\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -10762514857984.0000\n",
      "60/60 [==============================] - 11s 52ms/step - loss: -271757411876864.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -850836041236480.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -5733907504824320.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -1194642061005946880.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -1055742756913152.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -478572331594153984.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -1583333.87506.\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -7268896110238760960.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -15271902049483096064.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -146226576818700288.0000\n",
      "60/60 [==============================] - 8s 28ms/step - loss: -7566102899319635968.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -776352172391530496.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: nan951740346368.00\n",
      "60/60 [==============================] - 9s 40ms/step - loss: nan3526005760.00\n",
      "60/60 [==============================] - 7s 30ms/step - loss: nan601693184.00\n",
      "60/60 [==============================] - 9s 34ms/step - loss: nan730734518272.00\n",
      "60/60 [==============================] - 7s 30ms/step - loss: nan7949698170880.00\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -11318927949698170880.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -508419309568.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -42264634064896.0000\n",
      "60/60 [==============================] - 9s 45ms/step - loss: -6464847050964992.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -11182658172223488.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: 276423770112.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -45712754270535680.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -97382370482585600.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -560730142524047360.0000\n",
      "60/60 [==============================] - 8s 26ms/step - loss: -1814763.37502.00\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -4391316950186196992.0000\n",
      "60/60 [==============================] - 8s 32ms/step - loss: -2258639.250088.00\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -7824828980451606528.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -12051836012494585856.0000\n",
      "60/60 [==============================] - 12s 67ms/step - loss: -15980737404700983296.0000\n",
      "60/60 [==============================] - 12s 70ms/step - loss: nan98496415744.00\n",
      "60/60 [==============================] - 10s 62ms/step - loss: -5496410798496415744.0000\n",
      "60/60 [==============================] - 11s 65ms/step - loss: nan                 \n",
      "60/60 [==============================] - 10s 35ms/step - loss: nan44413679616.00\n",
      "60/60 [==============================] - 10s 26ms/step - loss: nan \n",
      "60/60 [==============================] - 10s 26ms/step - loss: nan\n",
      "60/60 [==============================] - 11s 35ms/step - loss: 30318820065280.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: 1019510128640.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -2705551834218496.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: 354967584.0000\n",
      "60/60 [==============================] - 14s 42ms/step - loss: -1769659535720448.0000\n",
      "60/60 [==============================] - 12s 53ms/step - loss: -770826.75000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -394474535476264960.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -429176221960503296.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -989420895741149184.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -704534478046887936.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -7652432154286096384.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -2129208.0000000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: 7900258816.0000.00\n",
      "60/60 [==============================] - 9s 38ms/step - loss: nan308059889664.00\n",
      "60/60 [==============================] - 8s 29ms/step - loss: -25659938375129366528.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -11383717771876499456.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -304404416.00000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: nan845106663424.00\n",
      "60/60 [==============================] - 9s 36ms/step - loss: nan920.000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: 10781559808.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -102416087252992.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -1186241708032.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -22593754917502976.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -733509614305280.0000\n",
      "60/60 [==============================] - 10s 32ms/step - loss: -2884816.7500\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -134668931494313984.0000\n",
      "60/60 [==============================] - 8s 32ms/step - loss: -291885290241392640.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -11228055.0000.00\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -12151015259854864384.0000\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -6690166940893184.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: 51998420992.00000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -90878544.0000.000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -3194825098305470464.0000\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -8769511778850701312.0000\n",
      "60/60 [==============================] - 7s 18ms/step - loss: nan553799700480.00\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -10544309913302073344.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: nan641126400.00    \n",
      "60/60 [==============================] - 10s 39ms/step - loss: nan \n",
      "60/60 [==============================] - 9s 36ms/step - loss: nan\n",
      "60/60 [==============================] - 8s 32ms/step - loss: nan\n",
      "60/60 [==============================] - 8s 28ms/step - loss: -48428184764416.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: 6301706354688.0000\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -657581102071808.0000\n",
      "60/60 [==============================] - 8s 40ms/step - loss: -127857620168998912.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -2065945841369088.0000\n",
      "60/60 [==============================] - 9s 43ms/step - loss: -309726137712377856.0000\n",
      "60/60 [==============================] - 9s 47ms/step - loss: -117015147028938752.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -1057811343622537216.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -2590057694022860800.0000\n",
      "60/60 [==============================] - 8s 37ms/step - loss: -1620654025940664320.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -161776024877531136.0000\n",
      "60/60 [==============================] - 8s 33ms/step - loss: nan86687246336.00\n",
      "60/60 [==============================] - 12s 66ms/step - loss: -1651398295198629888.0000\n",
      "60/60 [==============================] - 10s 63ms/step - loss: -476908804861067264.0000\n",
      "60/60 [==============================] - 11s 25ms/step - loss: nan40279330816.00\n",
      "60/60 [==============================] - 9s 26ms/step - loss: nan                  \n",
      "60/60 [==============================] - 11s 32ms/step - loss: nan           \n",
      "60/60 [==============================] - 12s 40ms/step - loss: nan            \n",
      "60/60 [==============================] - 11s 43ms/step - loss: nan       \n",
      "60/60 [==============================] - 12s 47ms/step - loss: nan250\n",
      "60/60 [==============================] - 8s 42ms/step - loss: -4996853.5000\n",
      "60/60 [==============================] - 12s 39ms/step - loss: -1409583.1250\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -2589506.0000\n",
      "60/60 [==============================] - 11s 52ms/step - loss: -4489670.5000\n",
      "60/60 [==============================] - 15s 46ms/step - loss: 755447.0625\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -10708052.0000\n",
      "60/60 [==============================] - 9s 45ms/step - loss: -15808624.0000\n",
      "60/60 [==============================] - 12s 51ms/step - loss: -11646666.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -12575272.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -17853364.0000\n",
      "60/60 [==============================] - 11s 40ms/step - loss: -17355860.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -24944916.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -25116650.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -26768374.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -27967540.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -29381464.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -33527106.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -40759180.0000\n",
      "60/60 [==============================] - 11s 51ms/step - loss: -31830940.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -28945224.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -2119624.7500\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -506029.2500\n",
      "60/60 [==============================] - 9s 28ms/step - loss: -2228044.5000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -3896317.7500\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -5878302.5000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: -10020798.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -12727170.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -13840331.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -15669428.0000\n",
      "60/60 [==============================] - 8s 40ms/step - loss: -20268570.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -19306262.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -16873260.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -26689508.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -27210942.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -31216294.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -34213108.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -30685530.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -43990232.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -29664374.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -39149704.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -463131.8125\n",
      "60/60 [==============================] - 8s 26ms/step - loss: -4389329.5000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -3499990.5000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -4434962.0000\n",
      "60/60 [==============================] - 8s 34ms/step - loss: -9393268.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -11064978.0000\n",
      "60/60 [==============================] - 8s 26ms/step - loss: -15069284.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -15867797.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -14109297.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -13257785.0000\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -21065080.0000\n",
      "60/60 [==============================] - 11s 41ms/step - loss: -23826740.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -27876992.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -26297230.0000\n",
      "60/60 [==============================] - 8s 33ms/step - loss: -30454106.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -24332504.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -34933688.0000\n",
      "60/60 [==============================] - 10s 31ms/step - loss: -35273552.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -33882652.0000\n",
      "60/60 [==============================] - 11s 68ms/step - loss: -35663792.0000\n",
      "60/60 [==============================] - 10s 63ms/step - loss: -3245269.0000\n",
      "60/60 [==============================] - 12s 66ms/step - loss: -189216.7500\n",
      "60/60 [==============================] - 9s 25ms/step - loss: -5162598.0000\n",
      "60/60 [==============================] - 11s 30ms/step - loss: -3439765.5000\n",
      "60/60 [==============================] - 11s 27ms/step - loss: -5380603.5000\n",
      "60/60 [==============================] - 11s 37ms/step - loss: -13096016.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -16500910.0000\n",
      "60/60 [==============================] - 11s 33ms/step - loss: -15829914.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -19203086.0000\n",
      "60/60 [==============================] - 11s 33ms/step - loss: -20301178.0000\n",
      "60/60 [==============================] - 15s 44ms/step - loss: -9106138.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: -23919798.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -21686920.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -26633820.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -28001846.0000\n",
      "60/60 [==============================] - 8s 32ms/step - loss: -28256896.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -28156458.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -34569784.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -35162484.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: 108629.9219\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -43097408.0000\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -2258537.5000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -3664342.7500\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -4433901.5000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -4784493.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -11653827.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -12582425.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -14700341.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -14363651.0000\n",
      "60/60 [==============================] - 8s 27ms/step - loss: -16242615.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -23420596.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -20300438.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -28226426.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -26279476.0000\n",
      "60/60 [==============================] - 7s 32ms/step - loss: -32002392.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -28092554.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -32505640.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: -36255700.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -34384548.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -37218160.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: 362187.7812\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -1717009.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -3016603.7500\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -4989385.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -12034959.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -8148294.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -13866373.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -11463184.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -15609851.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -16296966.0000\n",
      "60/60 [==============================] - 9s 29ms/step - loss: -17781044.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -20238198.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -27524190.0000\n",
      "60/60 [==============================] - 8s 32ms/step - loss: -29673592.0000\n",
      "60/60 [==============================] - 9s 46ms/step - loss: -31092324.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -28249896.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -26762216.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -32212722.0000\n",
      "60/60 [==============================] - 8s 28ms/step - loss: -38924560.0000\n",
      "60/60 [==============================] - 8s 29ms/step - loss: -41604480.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: 670877.3125\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -2427386.7500\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -3781199.7500\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -10276835.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -5496936.5000\n",
      "60/60 [==============================] - 10s 50ms/step - loss: -9579172.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -9848889.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -10547247.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -13625774.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -18523270.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -15402824.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -24342636.0000\n",
      "60/60 [==============================] - 11s 39ms/step - loss: -21957588.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -22401250.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -28783594.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -25888450.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -34298876.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -37761628.0000\n",
      "60/60 [==============================] - 9s 29ms/step - loss: -31101832.0000\n",
      "60/60 [==============================] - 13s 84ms/step - loss: -39308584.0000\n",
      "60/60 [==============================] - 12s 77ms/step - loss: 55738.3320\n",
      "60/60 [==============================] - 12s 32ms/step - loss: -2265707.7500\n",
      "60/60 [==============================] - 12s 29ms/step - loss: -2411570.7500\n",
      "60/60 [==============================] - 11s 26ms/step - loss: -5194878.0000\n",
      "60/60 [==============================] - 11s 28ms/step - loss: -5341408.5000\n",
      "60/60 [==============================] - 10s 29ms/step - loss: -12192830.0000\n",
      "60/60 [==============================] - 12s 34ms/step - loss: -7186753.0000\n",
      "60/60 [==============================] - 10s 32ms/step - loss: -15609467.0000\n",
      "60/60 [==============================] - 10s 52ms/step - loss: -21837884.0000\n",
      "60/60 [==============================] - 9s 45ms/step - loss: -23107134.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -21847808.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -26582228.0000\n",
      "60/60 [==============================] - 16s 35ms/step - loss: -14955387.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -27862798.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -24321588.0000\n",
      "60/60 [==============================] - 10s 48ms/step - loss: -34202336.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -34296792.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -33186918.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -33505480.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -43854044.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -377542.4062\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -2634193.7500\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -6420294.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -5525170.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -6458533.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -10626554.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -13442328.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -20407078.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -15837988.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -19337292.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -19653616.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -25773004.0000\n",
      "60/60 [==============================] - 8s 44ms/step - loss: -32333574.0000\n",
      "60/60 [==============================] - 11s 38ms/step - loss: -27283820.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -26978292.0000\n",
      "60/60 [==============================] - 8s 33ms/step - loss: -31193094.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -32173342.0000\n",
      "60/60 [==============================] - 10s 48ms/step - loss: -33146046.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -37642412.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -41410600.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -171359.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -2171165.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -5430198.5000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -7701317.5000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -5254373.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -10688869.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -13119718.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -15524153.0000\n",
      "60/60 [==============================] - 11s 38ms/step - loss: -16771243.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -17072790.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -19765066.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -21544884.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -25855366.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -27434116.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -31662760.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -27280644.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -31168186.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -32789726.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -37920008.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -35590864.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: 68908.6641\n",
      "60/60 [==============================] - 8s 27ms/step - loss: -4625695.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -1878596.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -3739298.2500\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -8086389.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -10658695.0000\n",
      "60/60 [==============================] - 11s 53ms/step - loss: -12007440.0000\n",
      "60/60 [==============================] - 11s 52ms/step - loss: -10465258.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -16061919.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -22355986.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -20640496.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -27355390.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -22956142.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -33434104.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -21591552.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -29652346.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -26195204.0000\n",
      "60/60 [==============================] - 10s 33ms/step - loss: -35227792.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -36868156.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -42482460.0000\n",
      "60/60 [==============================] - 11s 52ms/step - loss: 632913.0625\n",
      "60/60 [==============================] - 11s 52ms/step - loss: -545037.6875\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -3417807.5000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -6948315.5000\n",
      "60/60 [==============================] - 10s 50ms/step - loss: -8281932.5000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -9470708.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -12378411.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -9415991.0000\n",
      "60/60 [==============================] - 10s 33ms/step - loss: -19949554.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -21133504.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -16685198.0000\n",
      "60/60 [==============================] - 13s 93ms/step - loss: -25090886.0000\n",
      "60/60 [==============================] - 14s 106ms/step - loss: -18816176.0000\n",
      "60/60 [==============================] - 13s 95ms/step - loss: -26520640.0000\n",
      "60/60 [==============================] - 14s 45ms/step - loss: -28259138.0000\n",
      "60/60 [==============================] - 15s 49ms/step - loss: -33949204.0000\n",
      "60/60 [==============================] - 12s 36ms/step - loss: -35301428.0000\n",
      "60/60 [==============================] - 13s 39ms/step - loss: -33256236.0000\n",
      "60/60 [==============================] - 12s 44ms/step - loss: -42080392.0000\n",
      "60/60 [==============================] - 13s 43ms/step - loss: 941990.6875\n",
      "60/60 [==============================] - 13s 38ms/step - loss: -2405596.0000\n",
      "60/60 [==============================] - 12s 31ms/step - loss: -5998303.0000\n",
      "60/60 [==============================] - 19s 41ms/step - loss: -30911496.0000\n",
      "60/60 [==============================] - 13s 38ms/step - loss: -4885023.5000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -5044878.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -8442548.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -13512057.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -13616556.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -18447074.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -16497446.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -22176642.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -21131948.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -28190140.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -25983870.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -27252322.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -28785122.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -34926700.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -39860060.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -33171872.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -36920532.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: 457324.1875\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -1318416.8750\n",
      "60/60 [==============================] - 8s 29ms/step - loss: -6142690.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -4193054.2500\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -6816837.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -8381856.5000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -10618303.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -15308835.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -19183488.0000\n",
      "60/60 [==============================] - 8s 34ms/step - loss: -23904902.0000\n",
      "60/60 [==============================] - 12s 46ms/step - loss: -15549967.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -17472584.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -26260922.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -25840406.0000\n",
      "60/60 [==============================] - 10s 48ms/step - loss: -25231952.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -32497090.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -33782816.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -33458542.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -39099772.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -64529.5703\n",
      "60/60 [==============================] - 11s 39ms/step - loss: -31474986.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -6603278.5000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -3392022.2500\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -8929005.0000\n",
      "60/60 [==============================] - 9s 28ms/step - loss: -15942523.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -12216264.0000\n",
      "60/60 [==============================] - 12s 41ms/step - loss: -11334690.0000\n",
      "60/60 [==============================] - 9s 28ms/step - loss: -17156608.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -18803510.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -23528376.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -21821878.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -23584810.0000\n",
      "60/60 [==============================] - 9s 28ms/step - loss: -31449178.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -27379628.0000\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -30553214.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -38196664.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -33780704.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -35810312.0000\n",
      "60/60 [==============================] - 10s 50ms/step - loss: -39154172.0000\n",
      "60/60 [==============================] - 10s 49ms/step - loss: -139844.7969\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -42405784.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -3770362.2500\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -6853919.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -8697965.0000\n",
      "60/60 [==============================] - 12s 56ms/step - loss: -11034722.0000\n",
      "60/60 [==============================] - 12s 56ms/step - loss: -13757456.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -15461607.0000\n",
      "60/60 [==============================] - 11s 55ms/step - loss: -16402767.0000\n",
      "60/60 [==============================] - 10s 53ms/step - loss: -19810792.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -19379566.0000\n",
      "60/60 [==============================] - 10s 49ms/step - loss: -25760960.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -28102604.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -23156146.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -28396066.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -29213262.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -33380304.0000\n",
      "60/60 [==============================] - 11s 51ms/step - loss: -32836372.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -36757940.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -39373192.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -42576128.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: 135758.4688\n",
      "60/60 [==============================] - 9s 29ms/step - loss: -5932857.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -2661874.0000\n",
      "60/60 [==============================] - 8s 29ms/step - loss: -10135139.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -6781754.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -12682622.0000\n",
      "60/60 [==============================] - 9s 43ms/step - loss: -14765859.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -17735754.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -20136404.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -21124508.0000\n",
      "60/60 [==============================] - 9s 47ms/step - loss: -21662626.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -24261244.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -31168950.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -27249160.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -29781804.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -36951176.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -32024378.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -37743792.0000\n",
      "60/60 [==============================] - 9s 43ms/step - loss: -35426212.0000\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -3155471.7500\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -41503292.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -260883.8438\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -6232222.0000\n",
      "60/60 [==============================] - 13s 94ms/step - loss: -7676890.5000\n",
      "60/60 [==============================] - 13s 96ms/step - loss: -10498208.0000\n",
      "60/60 [==============================] - 13s 92ms/step - loss: -12925364.0000\n",
      "60/60 [==============================] - 13s 33ms/step - loss: -16987078.0000\n",
      "60/60 [==============================] - 11s 25ms/step - loss: -18688546.0000\n",
      "60/60 [==============================] - 12s 26ms/step - loss: -17489410.0000\n",
      "60/60 [==============================] - 11s 24ms/step - loss: -22639436.0000\n",
      "60/60 [==============================] - 12s 32ms/step - loss: -23113682.0000\n",
      "60/60 [==============================] - 13s 39ms/step - loss: -22615912.0000\n",
      "60/60 [==============================] - 13s 43ms/step - loss: -28391472.0000\n",
      "60/60 [==============================] - 8s 34ms/step - loss: -32305536.0000\n",
      "60/60 [==============================] - 13s 42ms/step - loss: -28299666.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -33232226.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -36910540.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -40817072.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -35458852.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -30070228.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: 182785.8125\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -8971565.0000\n",
      "60/60 [==============================] - 11s 41ms/step - loss: -3181489.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -6046893.5000\n",
      "60/60 [==============================] - 10s 33ms/step - loss: -10900681.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -11555366.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -14024976.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -17301990.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -19888566.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -23023184.0000\n",
      "60/60 [==============================] - 10s 31ms/step - loss: -20046822.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -25252050.0000\n",
      "60/60 [==============================] - 8s 37ms/step - loss: -31560626.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -28906744.0000\n",
      "60/60 [==============================] - 12s 49ms/step - loss: -28226570.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -32467732.0000\n",
      "60/60 [==============================] - 8s 32ms/step - loss: -37924696.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -32945706.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -33991108.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -29054.9746\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -41058940.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -2159581.5000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -11148443.0000\n",
      "60/60 [==============================] - 11s 38ms/step - loss: -5936187.5000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -8772395.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -16165265.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -12846121.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -16896728.0000\n",
      "60/60 [==============================] - 8s 34ms/step - loss: -23451200.0000\n",
      "60/60 [==============================] - 12s 50ms/step - loss: -19986502.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -20364412.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -23761108.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -28013342.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -30571048.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -35815392.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -31687444.0000\n",
      "60/60 [==============================] - 11s 41ms/step - loss: -35225392.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -37462504.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: 139591.9062\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -38515084.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -6862542.0000\n",
      "60/60 [==============================] - 12s 45ms/step - loss: -39284288.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -4201250.5000\n",
      "60/60 [==============================] - 11s 52ms/step - loss: -9014242.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -12617450.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -9995219.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -18159458.0000\n",
      "60/60 [==============================] - 11s 38ms/step - loss: -15145964.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -21400008.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -25220756.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -20206258.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -27437458.0000\n",
      "60/60 [==============================] - 11s 40ms/step - loss: -25362020.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -31378612.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -30813718.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -33811268.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -35354568.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -37679480.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -36531520.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -42259448.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -2567134.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -237672.8906\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -8517124.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -6294128.5000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -10960007.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -14744393.0000\n",
      "60/60 [==============================] - 8s 34ms/step - loss: -18373028.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -15796841.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -17898720.0000\n",
      "60/60 [==============================] - 9s 43ms/step - loss: -22586760.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -21475780.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -26222092.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -27722352.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -31925354.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -29145552.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -32912634.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -35308788.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -38071172.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -41604368.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -39999800.0000\n",
      "60/60 [==============================] - 10s 50ms/step - loss: 10777.9854\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -3517025.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -7018914.5000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -6741946.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -10397451.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -13191780.0000\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -17513348.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -15114692.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -18933768.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -19044350.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -22476722.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -28365874.0000\n",
      "60/60 [==============================] - 12s 51ms/step - loss: -25575258.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -29334474.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -33206500.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -32173738.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -35313192.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -35680068.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -40986468.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -40783820.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -3194784.2500\n",
      "60/60 [==============================] - 11s 37ms/step - loss: -773377.2500\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -6524829.0000\n",
      "60/60 [==============================] - 11s 39ms/step - loss: -7628066.5000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -13426676.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -9816453.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -13718585.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -19176338.0000\n",
      "60/60 [==============================] - 12s 53ms/step - loss: -18676462.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -22158636.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -20193422.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -31244648.0000\n",
      "60/60 [==============================] - 11s 38ms/step - loss: -26736832.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -32409094.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -26594622.0000\n",
      "60/60 [==============================] - 14s 111ms/step - loss: -37864888.0000\n",
      "60/60 [==============================] - 13s 102ms/step - loss: -39990968.0000\n",
      "60/60 [==============================] - 15s 113ms/step - loss: -36700844.0000\n",
      "60/60 [==============================] - 15s 36ms/step - loss: -37986644.0000\n",
      "60/60 [==============================] - 13s 31ms/step - loss: -40626608.0000\n",
      "60/60 [==============================] - 13s 28ms/step - loss: -273088.3750\n",
      "60/60 [==============================] - 13s 36ms/step - loss: -2431038.2500\n",
      "60/60 [==============================] - 13s 33ms/step - loss: -8395284.0000\n",
      "60/60 [==============================] - 13s 39ms/step - loss: -13953669.0000\n",
      "60/60 [==============================] - 14s 42ms/step - loss: -12258237.0000\n",
      "60/60 [==============================] - 9s 51ms/step - loss: -16242738.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -16280280.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -19358936.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -21139328.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -23267110.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -23414466.0000\n",
      "60/60 [==============================] - 23s 43ms/step - loss: -7289528.5000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -25008320.0000\n",
      "60/60 [==============================] - 11s 37ms/step - loss: -29728638.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -32327842.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -33472834.0000\n",
      "60/60 [==============================] - 11s 40ms/step - loss: -33282650.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: -39300080.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -38247172.0000\n",
      "60/60 [==============================] - 11s 35ms/step - loss: -37799304.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -453998.6562\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -3048703.2500\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -6328366.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -7909819.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -10645295.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -15676278.0000\n",
      "60/60 [==============================] - 12s 53ms/step - loss: -12364881.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -18996660.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -19032728.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -24579616.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -22237792.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -26757688.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -29215892.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -30734274.0000\n",
      "60/60 [==============================] - 10s 32ms/step - loss: -31447678.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -31219070.0000\n",
      "60/60 [==============================] - 9s 29ms/step - loss: -35849868.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -37863176.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -40932812.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -41408740.0000\n",
      "60/60 [==============================] - 10s 48ms/step - loss: -154691.1719\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -3727180.7500\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -6925985.5000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -7738676.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -12861663.0000\n",
      "60/60 [==============================] - 8s 40ms/step - loss: -14379001.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -10182149.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -17311798.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -20163680.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -19897122.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -21864998.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -25594714.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -24405310.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -30681082.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -30299634.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -34626108.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -34171900.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -37055272.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -38317840.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -57845.0742\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -41697464.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -3843137.2500\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -6538562.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -8227446.5000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -12853743.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -11041457.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -14276068.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -20023468.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -17255798.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -20980488.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -21121790.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -30147748.0000\n",
      "60/60 [==============================] - 8s 32ms/step - loss: -30567032.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -24704166.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -28107636.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -33679940.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -39140424.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -33946992.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -39293992.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -42374272.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -4432587128832.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -934736.750000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -9093796532846592.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -34919434883694592.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -137254424497094656.0000\n",
      "60/60 [==============================] - 9s 29ms/step - loss: -188788826548731904.0000\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -422067638768762880.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -3000773390630912000.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -582479616392822784.0000\n",
      "60/60 [==============================] - 11s 56ms/step - loss: -73473568.00000\n",
      "60/60 [==============================] - 11s 61ms/step - loss: -13593125914752843776.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -10059463768359829504.0000\n",
      "60/60 [==============================] - 10s 48ms/step - loss: nan859118542848.00\n",
      "60/60 [==============================] - 10s 53ms/step - loss: -5535595193887096832.0000\n",
      "60/60 [==============================] - 9s 50ms/step - loss: nan\n",
      "60/60 [==============================] - 11s 31ms/step - loss: nan257504256.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: nan211928064.00\n",
      "60/60 [==============================] - 8s 31ms/step - loss: nan48296927232.\n",
      "60/60 [==============================] - 10s 36ms/step - loss: nan71265460224.\n",
      "60/60 [==============================] - 9s 35ms/step - loss: nan407296.00\n",
      "60/60 [==============================] - 10s 37ms/step - loss: 791883087872.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -449488762175488.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -24499056097099776.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -699553938407424.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -385388549341446144.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -697402014836981760.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -604838322782601216.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -8923465.00000.00\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -1274292057711575040.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -2654057791974539264.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -3034122128057171968.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: nan210032906240.00\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -39315386807967809536.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: nan201436086272.00\n",
      "60/60 [==============================] - 11s 48ms/step - loss: nan3478258688.00\n",
      "60/60 [==============================] - 10s 40ms/step - loss: nan057600.0056.00\n",
      "60/60 [==============================] - 11s 42ms/step - loss: nan90333085696.00\n",
      "60/60 [==============================] - 10s 45ms/step - loss: nan048.00368576.00\n",
      "60/60 [==============================] - 12s 51ms/step - loss: nan22.0\n",
      "60/60 [==============================] - 10s 48ms/step - loss: nan0.000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -9828.5947\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -2834431737856.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -56008403582976.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -43060010669834240.0000\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -69068962519842816.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -211793255501660160.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -230939223274815488.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -1411388238433615872.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -8006979574267117568.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -3055008725816311808.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -8281927849955295232.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -13817243767888936960.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: nan822950440960.00\n",
      "60/60 [==============================] - 11s 39ms/step - loss: nan201512325120.\n",
      "60/60 [==============================] - 10s 39ms/step - loss: nan773452709888.00\n",
      "60/60 [==============================] - 10s 38ms/step - loss: nan6708224.00\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -17071040.000000 \n",
      "60/60 [==============================] - 9s 31ms/step - loss: nan1765277786112.\n",
      "60/60 [==============================] - 10s 38ms/step - loss: nan821723684864.00\n",
      "60/60 [==============================] - 9s 32ms/step - loss: nan70893568.00\n",
      "60/60 [==============================] - 8s 31ms/step - loss: -976086145957888.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -8307447613620224.0000\n",
      "60/60 [==============================] - 11s 41ms/step - loss: -8744.8828\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -21727410621775872.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -11786446822178816.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -1020326655769444352.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -672214402467889152.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -6554115695649488896.0000\n",
      "60/60 [==============================] - 11s 41ms/step - loss: -1136841147050622976.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -10751849330114560000.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -4485748506337738752.0000\n",
      "60/60 [==============================] - 15s 115ms/step - loss: nan60617601024.00\n",
      "60/60 [==============================] - 15s 116ms/step - loss: nan7834086400.00\n",
      "60/60 [==============================] - 14s 118ms/step - loss: nan67134720.00\n",
      "60/60 [==============================] - 15s 29ms/step - loss: nan4359633920.00\n",
      "60/60 [==============================] - 14s 27ms/step - loss: -2788638.2500.\n",
      "60/60 [==============================] - 14s 33ms/step - loss: nan48210688.00\n",
      "60/60 [==============================] - 14s 32ms/step - loss: nan0967846912.00\n",
      "60/60 [==============================] - 15s 38ms/step - loss: nan504.00\n",
      "60/60 [==============================] - 14s 38ms/step - loss: nan024.00\n",
      "60/60 [==============================] - 14s 43ms/step - loss: -49659.0156\n",
      "60/60 [==============================] - 14s 43ms/step - loss: 148484079616.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -70998971908096.0000\n",
      "60/60 [==============================] - 12s 56ms/step - loss: -127349130400890880.0000\n",
      "60/60 [==============================] - 11s 53ms/step - loss: -248629197134626816.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: 32568494080.0000\n",
      "60/60 [==============================] - 11s 51ms/step - loss: -1090699316957282304.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -17833500259410509824.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -17528013048260853760.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: nan0746496.0000 \n",
      "60/60 [==============================] - 11s 44ms/step - loss: -4801514128277504.0000\n",
      "60/60 [==============================] - 12s 52ms/step - loss: nan488.000076.00\n",
      "60/60 [==============================] - 11s 52ms/step - loss: 93437657088.000000\n",
      "60/60 [==============================] - 12s 47ms/step - loss: nan73075200.00\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -24202870.000000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: nan2480461824.00\n",
      "60/60 [==============================] - 11s 47ms/step - loss: nan84827648.000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: nan990777856.00\n",
      "60/60 [==============================] - 10s 35ms/step - loss: nan0008.0\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -3421830.7500\n",
      "60/60 [==============================] - 10s 42ms/step - loss: 1937154768896.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: 402067652608.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -3361913299795968.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -27613601023918080.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -162859868464611328.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -12916636327608320.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -5413245937994694656.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -1367815142380666880.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -4872377476594008064.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -10036569737246277632.0000\n",
      "60/60 [==============================] - 10s 51ms/step - loss: -25081100478686691328.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -2151467.5000.0\n",
      "60/60 [==============================] - 10s 46ms/step - loss: nan.0070174720.000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: 282809344.0000.000\n",
      "60/60 [==============================] - 8s 36ms/step - loss: nan232214069248.00\n",
      "60/60 [==============================] - 10s 39ms/step - loss: nan68252704768.00\n",
      "60/60 [==============================] - 9s 30ms/step - loss: nan7006976.\n",
      "60/60 [==============================] - 11s 46ms/step - loss: nan.00\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -52526856.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: nan\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -3641.9932.000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: 237235617792.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -29935348674985984.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -7448515894575104.0000\n",
      "60/60 [==============================] - 8s 36ms/step - loss: -46009708309381120.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: 66547187712.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -2978838958290501632.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -3318813176279072768.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -6088114630251511808.0000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -1895289.87506.00\n",
      "60/60 [==============================] - 9s 35ms/step - loss: nan0908032.00     \n",
      "60/60 [==============================] - 9s 32ms/step - loss: nan776.0000352.000\n",
      "60/60 [==============================] - 7s 29ms/step - loss: nan016124035072.00\n",
      "60/60 [==============================] - 9s 39ms/step - loss: 60677877760.0000.000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: nan                \n",
      "60/60 [==============================] - 9s 31ms/step - loss: nan074912604160.00\n",
      "60/60 [==============================] - 9s 40ms/step - loss: nan586646765568.00\n",
      "60/60 [==============================] - 9s 29ms/step - loss: nan301440.\n",
      "60/60 [==============================] - 9s 30ms/step - loss: nan27752704.00\n",
      "60/60 [==============================] - 9s 37ms/step - loss: nan0611968.00\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -149299136561152.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -3694.4868000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -670548547862528.0000\n",
      "60/60 [==============================] - 9s 43ms/step - loss: -6595736716181504.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -125773564598026240.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -432300965287165952.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: 52257378304.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -7244489.00000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -87486088.0000.000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -947600489942351872.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -7630721747439845376.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -40929034470938378240.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -14989336357237686272.0000\n",
      "60/60 [==============================] - 8s 43ms/step - loss: -73349144.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: nan247966208.\n",
      "60/60 [==============================] - 11s 50ms/step - loss: nan62526213376.\n",
      "60/60 [==============================] - 10s 42ms/step - loss: nan78657220608.00\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -34723204.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: nan50400.00\n",
      "60/60 [==============================] - 9s 37ms/step - loss: nan268992.00\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -1715733397504.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -250674289836032.0000\n",
      "60/60 [==============================] - 9s 30ms/step - loss: -12302007180197888.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -18205204579090432.0000\n",
      "60/60 [==============================] - 10s 33ms/step - loss: -153312173345472512.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -597632535891017728.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -1133539382351888384.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -1100219644544811008.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -1537748.62500\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -9482081625312329728.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -2846666390637641728.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -146508304.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: nan6014976.00000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: nan12.0000       \n",
      "60/60 [==============================] - 9s 42ms/step - loss: 12743518208.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: nan547141632.00\n",
      "60/60 [==============================] - 9s 38ms/step - loss: nan91667431424.\n",
      "60/60 [==============================] - 9s 34ms/step - loss: nan3920.00\n",
      "60/60 [==============================] - 10s 41ms/step - loss: 356414423040.0000\n",
      "60/60 [==============================] - 8s 38ms/step - loss: -385029.5938840.000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -34092.1602\n",
      "60/60 [==============================] - 11s 39ms/step - loss: nan664.00.0000 \n",
      "60/60 [==============================] - 10s 38ms/step - loss: -163690649223168.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -967423532466176.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: 61056995328.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -20262156.000000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -113996781452263424.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -3031437945295863808.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -757695521652473856.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -4684609477872189440.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -11343783509555675136.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -4199797692668313600.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: nan08918685696.00 \n",
      "60/60 [==============================] - 10s 41ms/step - loss: nan31856826368.00\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -2751957.2500    \n",
      "60/60 [==============================] - 9s 32ms/step - loss: nan788116992.00\n",
      "60/60 [==============================] - 11s 45ms/step - loss: nan98592.00044.000\n",
      "60/60 [==============================] - 11s 41ms/step - loss: 18958852096.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: nan6362058752.00\n",
      "60/60 [==============================] - 8s 37ms/step - loss: -1359537373184.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: nan7808.000\n",
      "60/60 [==============================] - 9s 39ms/step - loss: -1369902233419776.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -90159861202944.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -12130268617900032.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -2648448633405440.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -603268769933950976.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -4963017916652978176.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -2950427783987200.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -6053101781976612864.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -22007204617006350336.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: 52477546496.00000\n",
      "60/60 [==============================] - 11s 39ms/step - loss: nan\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -19374850634808295424.0000\n",
      "60/60 [==============================] - 8s 31ms/step - loss: nan000071331840.00\n",
      "60/60 [==============================] - 10s 39ms/step - loss: nan5000 \n",
      "60/60 [==============================] - 9s 33ms/step - loss: -2984952.7500\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -3264736.00006.00 \n",
      "60/60 [==============================] - 11s 46ms/step - loss: nan1486720.\n",
      "60/60 [==============================] - 9s 41ms/step - loss: nan19033984.00\n",
      "60/60 [==============================] - 10s 42ms/step - loss: nan736.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -5644.4453.000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: 107727159296.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -13648204549586944.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -573567.3125000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -172202246967132160.0000\n",
      "60/60 [==============================] - 11s 54ms/step - loss: -634243386877935616.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -916415728838508544.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -933341061080678400.0000\n",
      "60/60 [==============================] - 10s 50ms/step - loss: -581819359660343296.0000\n",
      "60/60 [==============================] - 8s 37ms/step - loss: nan584878407680.00\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -84971344.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: nan             \n",
      "60/60 [==============================] - 10s 33ms/step - loss: -10470677.0000.0000\n",
      "60/60 [==============================] - 11s 34ms/step - loss: nan47894454272.000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: nan6450347720704.00\n",
      "60/60 [==============================] - 9s 30ms/step - loss: nan                \n",
      "60/60 [==============================] - 9s 30ms/step - loss: 58124320768.00000\n",
      "60/60 [==============================] - 9s 25ms/step - loss: nan692589568.\n",
      "60/60 [==============================] - 9s 31ms/step - loss: nan1875352576.00\n",
      "60/60 [==============================] - 10s 41ms/step - loss: nan59456.00\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -308877807583232.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: 1040202203136.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -8416942972993536.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -386928.968800\n",
      "60/60 [==============================] - 8s 30ms/step - loss: -318802606199668736.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -782554.6250\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -1138231.750000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -9101432349043720192.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -8512761520113844224.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -11093299367585513472.0000\n",
      "60/60 [==============================] - 12s 53ms/step - loss: 18673242112.0000\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -23189672198074793984.0000\n",
      "60/60 [==============================] - 11s 53ms/step - loss: -2283849.00000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -2202488091569029120.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: nan1523300352.00\n",
      "60/60 [==============================] - 11s 47ms/step - loss: nan8153133056.\n",
      "60/60 [==============================] - 17s 143ms/step - loss: -6175324594031820800.0000\n",
      "60/60 [==============================] - 17s 31ms/step - loss: nan2058678272.00\n",
      "60/60 [==============================] - 15s 34ms/step - loss: nan90317389824.00\n",
      "60/60 [==============================] - 17s 43ms/step - loss: nan10720.00\n",
      "60/60 [==============================] - 16s 41ms/step - loss: 3398379503616.0000\n",
      "60/60 [==============================] - 15s 42ms/step - loss: 743552384.0000\n",
      "60/60 [==============================] - 17s 45ms/step - loss: -271702231613440.0000\n",
      "60/60 [==============================] - 16s 44ms/step - loss: -3449506473443328.0000\n",
      "60/60 [==============================] - 17s 46ms/step - loss: -118818354688425984.0000\n",
      "60/60 [==============================] - 16s 40ms/step - loss: -100429185922629632.0000\n",
      "60/60 [==============================] - 9s 44ms/step - loss: -2216844964648714240.0000\n",
      "60/60 [==============================] - 9s 49ms/step - loss: -3405358210402484224.0000\n",
      "60/60 [==============================] - 11s 52ms/step - loss: -5534027840061702144.0000\n",
      "60/60 [==============================] - 12s 64ms/step - loss: -502840614803472384.0000\n",
      "60/60 [==============================] - 11s 58ms/step - loss: nan65253632.0000\n",
      "60/60 [==============================] - 13s 62ms/step - loss: -24606408323626958848.0000\n",
      "60/60 [==============================] - 12s 54ms/step - loss: -2553111.50000000\n",
      "60/60 [==============================] - 12s 45ms/step - loss: -4127586991636217856.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: nan811452395520.00\n",
      "60/60 [==============================] - 11s 53ms/step - loss: -12210785811452395520.0000\n",
      "60/60 [==============================] - 26s 45ms/step - loss: -403959128935890944.0000\n",
      "60/60 [==============================] - 11s 39ms/step - loss: nan2.00\n",
      "60/60 [==============================] - 11s 38ms/step - loss: -315892320.0000\n",
      "60/60 [==============================] - 12s 39ms/step - loss: -3798170.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -3608536.5000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -469281.6250\n",
      "60/60 [==============================] - 11s 54ms/step - loss: -10090316.0000\n",
      "60/60 [==============================] - 12s 51ms/step - loss: -5457728.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -12026125.0000\n",
      "60/60 [==============================] - 11s 47ms/step - loss: -13731448.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -16526086.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -26824004.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -21919344.0000\n",
      "60/60 [==============================] - 11s 36ms/step - loss: -22719278.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -30595390.0000\n",
      "60/60 [==============================] - 11s 39ms/step - loss: -28975304.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -36027948.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -38778240.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -36576636.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -40874616.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -46865320.0000\n",
      "60/60 [==============================] - 8s 35ms/step - loss: -47509952.0000\n",
      "60/60 [==============================] - 10s 50ms/step - loss: -58505772.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: 290096.2812\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -51387700.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -1290988.8750\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -8121307.5000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -3078947.2500\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -6956689.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -15857923.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -20337374.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -17007096.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -20128636.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -26372656.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -18972106.0000\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -24998070.0000\n",
      "60/60 [==============================] - 10s 35ms/step - loss: -29331554.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -30554584.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -39559156.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -43592380.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -48361816.0000\n",
      "60/60 [==============================] - 11s 51ms/step - loss: -47119724.0000\n",
      "60/60 [==============================] - 10s 49ms/step - loss: -46882196.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -53111744.0000\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -319134.7812\n",
      "60/60 [==============================] - 9s 35ms/step - loss: -8832918.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -6078394.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -2052479.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -12503885.0000\n",
      "60/60 [==============================] - 9s 41ms/step - loss: -14302974.0000\n",
      "60/60 [==============================] - 10s 31ms/step - loss: -14807298.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -22120778.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -21696174.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -28933972.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -23922748.0000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -32465122.0000\n",
      "60/60 [==============================] - 10s 38ms/step - loss: -34391424.0000\n",
      "60/60 [==============================] - 9s 31ms/step - loss: -38743412.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -39613572.0000\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -44328516.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -44662120.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -49836080.0000\n",
      "60/60 [==============================] - 9s 43ms/step - loss: -53935664.0000\n",
      "60/60 [==============================] - 9s 32ms/step - loss: -563328.5625\n",
      "60/60 [==============================] - 9s 45ms/step - loss: -59991140.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -2945895.5000\n",
      "60/60 [==============================] - 10s 40ms/step - loss: -9187108.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -5865350.0000\n",
      "60/60 [==============================] - 11s 51ms/step - loss: -10111014.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -11552546.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -16582353.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -20523088.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -24965826.0000\n",
      "60/60 [==============================] - 10s 48ms/step - loss: -27817514.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -29420768.0000\n",
      "60/60 [==============================] - 9s 42ms/step - loss: -35267432.0000\n",
      "60/60 [==============================] - 11s 54ms/step - loss: -33649488.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -33333344.0000\n",
      "60/60 [==============================] - 9s 33ms/step - loss: -48759952.0000\n",
      "60/60 [==============================] - 11s 51ms/step - loss: -41558044.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -47336256.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -54761712.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -52497636.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -51655172.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: 38727.5039\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -4607743.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -3159348.7500\n",
      "60/60 [==============================] - 11s 49ms/step - loss: -6854712.5000\n",
      "60/60 [==============================] - 8s 37ms/step - loss: -19921934.0000\n",
      "60/60 [==============================] - 11s 51ms/step - loss: -8523703.0000\n",
      "60/60 [==============================] - 10s 50ms/step - loss: -14617911.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -19696900.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -25894622.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -29107546.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -29318020.0000\n",
      "60/60 [==============================] - 12s 48ms/step - loss: -23843912.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -37823320.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -35598896.0000\n",
      "60/60 [==============================] - 10s 36ms/step - loss: -43041292.0000\n",
      "60/60 [==============================] - 11s 48ms/step - loss: -41148296.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -48259492.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -43188744.0000\n",
      "60/60 [==============================] - 10s 47ms/step - loss: -48447704.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -58947944.0000\n",
      "60/60 [==============================] - 11s 44ms/step - loss: -222887.5938\n",
      "60/60 [==============================] - 9s 37ms/step - loss: -6397715.5000\n",
      "60/60 [==============================] - 12s 46ms/step - loss: -4364340.0000\n",
      "60/60 [==============================] - 9s 40ms/step - loss: -12201862.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -8323508.5000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -13431946.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -11461844.0000\n",
      "60/60 [==============================] - 11s 38ms/step - loss: -17749466.0000\n",
      "60/60 [==============================] - 11s 38ms/step - loss: -19385698.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -29467254.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -25602132.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -31253884.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -33471074.0000\n",
      "60/60 [==============================] - 11s 42ms/step - loss: -34581948.0000\n",
      "60/60 [==============================] - 10s 39ms/step - loss: -42454168.0000\n",
      "60/60 [==============================] - 11s 41ms/step - loss: -41661512.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -57343640.0000\n",
      "60/60 [==============================] - 11s 41ms/step - loss: -54160244.0000\n",
      "60/60 [==============================] - 9s 36ms/step - loss: -49903692.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -56372896.0000\n",
      "60/60 [==============================] - 9s 34ms/step - loss: -822053.0000\n",
      "60/60 [==============================] - 10s 44ms/step - loss: -2730692.2500\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -1222393.8750\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -8825204.0000\n",
      "60/60 [==============================] - 10s 46ms/step - loss: -9950379.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -18219150.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -21105460.0000\n",
      "60/60 [==============================] - 10s 42ms/step - loss: -19728006.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -17422692.0000\n",
      "60/60 [==============================] - 11s 43ms/step - loss: -28226812.0000\n",
      "60/60 [==============================] - 10s 34ms/step - loss: -32966082.0000\n",
      "60/60 [==============================] - 9s 38ms/step - loss: -33832940.0000\n",
      "60/60 [==============================] - 10s 45ms/step - loss: -34614792.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -34539948.0000\n",
      "60/60 [==============================] - 11s 54ms/step - loss: -40322500.0000\n",
      "60/60 [==============================] - 11s 45ms/step - loss: -46074912.0000\n",
      "60/60 [==============================] - 10s 43ms/step - loss: -52516468.0000\n",
      "60/60 [==============================] - 11s 46ms/step - loss: -50802944.0000\n",
      "60/60 [==============================] - 10s 37ms/step - loss: -55675036.0000\n",
      "60/60 [==============================] - 12s 57ms/step - loss: -53546164.0000\n",
      "60/60 [==============================] - 10s 41ms/step - loss: -518457.4688\n",
      "60/60 [==============================] - 12s 57ms/step - loss: -1812777.8750\n",
      "60/60 [==============================] - 11s 50ms/step - loss: -9097329.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "source_dir = r'C:\\Users\\bartlomiej.kielan\\Desktop\\codes\\azure-ml-internal-project\\data\\aggregated'\n",
    "csv_files = [file for file in os.listdir(source_dir) if file.endswith('.csv')]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in csv_files:\n",
    "    filepath = os.path.join(source_dir, filename)\n",
    "    temp_df = pd.read_csv(filepath)\n",
    "    dfs.append(temp_df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df.drop(['Country_Region'], axis=1, inplace=True)\n",
    "numeric_columns = combined_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "data_numeric = combined_df[numeric_columns]\n",
    "data_scaled = data_numeric.values\n",
    "\n",
    "async def sdae_reduction(data_scaled, dim_num=2, hyperparameters=None):\n",
    "    global best_loss\n",
    "    global best_hyperparameters\n",
    "\n",
    "    num_layers_values, layer_dim_values, activation_ch, epoch_num, batch_size_num, learning_rate_num = hyperparameters\n",
    "\n",
    "    neurons_per_layer = [\n",
    "        max(int(layer_dim_values / (2**i)), 2)\n",
    "        for i in range(num_layers_values)\n",
    "    ]\n",
    "\n",
    "    encoder_input = layers.Input(shape=(data_scaled.shape[1],))\n",
    "    encoder_output = encoder_input\n",
    "    for dim in neurons_per_layer[:-1]:\n",
    "        encoder_output = layers.Dense(dim, activation=activation_ch)(encoder_output)\n",
    "\n",
    "    encoded_layer = layers.Dense(dim_num, activation=activation_ch)(encoder_output)\n",
    "    encoder = keras.models.Model(encoder_input, encoded_layer)\n",
    "\n",
    "    decoder_input = layers.Input(shape=(dim_num,))\n",
    "    decoder_output = decoder_input\n",
    "    for dim in reversed(neurons_per_layer[:-1]):\n",
    "        decoder_output = layers.Dense(dim, activation=activation_ch)(decoder_output)\n",
    "    decoder_output = layers.Dense(data_scaled.shape[1], activation='sigmoid')(decoder_output)\n",
    "    decoder = keras.models.Model(decoder_input, decoder_output)\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    autoencoder_output = decoder(encoded_layer)\n",
    "    autoencoder = keras.models.Model(autoencoder_input, autoencoder_output)\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate_num)\n",
    "    autoencoder.compile(loss=\"binary_crossentropy\", optimizer=opt)\n",
    "\n",
    "    loss_values = []\n",
    "    epoch_numbers = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        history = await asyncio.to_thread(\n",
    "            autoencoder.fit,\n",
    "            data_scaled,\n",
    "            data_scaled,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size_num,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        loss_values.append(history.history['loss'][0])\n",
    "        epoch_numbers.append(epoch + 1)\n",
    "\n",
    "        if best_hyperparameters is None or history.history['loss'][0] < best_loss:\n",
    "            best_loss = history.history['loss'][0]\n",
    "            best_hyperparameters = {\n",
    "                'num_layers_values': num_layers_values,\n",
    "                'layer_dim_values': layer_dim_values,\n",
    "                'activation': activation_ch,\n",
    "                'epochs': epoch + 1,\n",
    "                'batch_size': batch_size_num,\n",
    "                'learning_rate': learning_rate_num\n",
    "            }\n",
    "\n",
    "async def main():\n",
    "    hyperparameter_grid = product(\n",
    "        [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        [10, 15, 20, 25, 30, 45],\n",
    "        [\"relu\", \"sigmoid\", \"tanh\"],\n",
    "        range(100, 1500, 100),\n",
    "        [128],\n",
    "        np.linspace(0.0001, 0.1, num=20)\n",
    "    )\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for hyperparameters in hyperparameter_grid:\n",
    "        task = asyncio.create_task(sdae_reduction(data_scaled, dim_num=2, hyperparameters=hyperparameters))\n",
    "        tasks.append(task)\n",
    "\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
